<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
  <style>
    /* bootstrap modal */

    .modal {
      text-align: center;
      padding: 0 !important;
    }

    .modal:before {
      content: '';
      display: inline-block;
      height: 100%;
      vertical-align: middle;
      margin-right: -4px;
    }

    .modal-dialog {
      display: inline-block;
      text-align: left;
      vertical-align: middle;
    }

    @media (min-width: 992px) {
      .modal-xl {
        max-width: 1400px;
      }
    }

    /* table of contents */

    @media (max-width: 1000px) {
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1000px) {
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }
  </style>

  <style id="distill-article-specific-styles">
    @media (min-height: 900px) {
      d-article hr {
        margin-top: 120px !important;
        margin-bottom: 100px !important;
      }
    }

    .subgrid {
      grid-column: screen;
      display: grid;
      grid-template-columns: inherit;
      grid-template-rows: inherit;
      grid-column-gap: inherit;
      grid-row-gap: inherit;
    }

    d-figure.base-grid {
      grid-column: screen;
      background: hsl(0, 0%, 97%);
      padding: 20px 0;
      border-top: 1px solid rgba(0, 0, 0, 0.1);
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    }

    d-figure {
      margin-bottom: 1em;
      position: relative;
    }

    d-figure>figure {
      margin-top: 0;
      margin-bottom: 0;
    }

    .shaded-figure {
      background-color: hsl(0, 0%, 97%);
      border-top: 1px solid hsla(0, 0%, 0%, 0.1);
      border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
      padding: 30px 0;
    }

    .pointer {
      position: absolute;
      width: 26px;
      height: 26px;
      top: 26px;
      left: -48px;
    }

    .fullscreen-diagram {
      grid-column: screen;
      background: #f8f8fb;
      padding: 10px;
      padding-top: 40px;
      padding-bottom: 40px;
      border-top: 1px solid #f0f0f0;
      border-bottom: 1px solid #f0f0f0;
      margin-top: 40px;
      margin-bottom: 60px;
    }

    .margin-diagram {
      grid-column: 12 / 15;
      grid-row: auto / span 3;
      margin-top: 6px;
    }

    @media (min-width: 1800px) {
      .margin-diagram {
        margin-left: 60px;
      }
    }

    d-footnote {
      margin-left: -1px;
      margin-right: 4px;
    }

    .footnote-sep {
      width: 0px;
    }

    .footnote-sep::before {
      line-height: 1em;
      top: -0.5em;
      font-size: 0.75em;
      color: #999;
      margin-left: -6px;
      margin-right: 1px;
      content: ',';
    }

    d-footnote[comma]::before {
      vertical-align: super;
      line-height: 1em;
      top: -0.5em;
      font-size: 0.75em;
      color: #999;
      margin-left: -6px;
      margin-right: 1px;
      content: ',';
    }

    d-article h2 {
      border-bottom: none;
    }

    /* ****************************************
     * TOC
     ******************************************/
    @media (max-width: 1300px) {
      d-contents {
        display: none;
      }
    }

    b i {
      display: inline;
    }

    d-article d-contents {
      align-self: start;
      grid-column: 1 / 4;
      grid-row: auto / span 4;
      justify-self: end;
      margin-top: 0em;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
    }

    d-contents a:hover {
      border-bottom: none;
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav div {
      color: rgba(0, 0, 0, 0.8);
      font-weight: bold;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }

    d-contents nav ul li {
      margin-bottom: 0.25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }

    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
      text-decoration: none;
    }

    d-article h2 {
      border-bottom: none !important;
    }

    d-article h3 {
      font-size: 26px !important;
    }
  </style>
  <style>
    .carousel .carousel-item {
      height: 550px;
    }

    .carousel-item img {
      position: absolute;
      object-fit: contain;
      top: 0;
      left: 0;
      max-height: 515px;
    }

    .carousel-control-prev,
    .carousel-control-next {
      border-bottom: 0px;
      opacity: 0.5;
    }

    .carousel-control-prev:hover,
    .carousel-control-next:hover {
      border-bottom: 0px;
      opacity: .9;
    }

    .carousel-caption {
      color: black;
      position: relative;
      left: auto;
      right: auto;
      top: 500px;
    }

    .carousel-control-prev-icon {
      background-image: url("data:image/svg+xml;charset=utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='%23c7c7c7' viewBox='0 0 8 8'%3E%3Cpath d='M5.25 0l-4 4 4 4 1.5-1.5-2.5-2.5 2.5-2.5-1.5-1.5z'/%3E%3C/svg%3E");
    }

    .carousel-control-next-icon {
      background-image: url("data:image/svg+xml;charset=utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='%23c7c7c7' viewBox='0 0 8 8'%3E%3Cpath d='M2.75 0l-1.5 1.5 2.5 2.5-2.5 2.5 1.5 1.5 4-4-4-4z'/%3E%3C/svg%3E");
    }
  </style>
</head>

<body>
  <distill-header></distill-header>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Of Deadly Skullcaps and Amethyst Deceivers",
    "description": "Reflections on a Transdisciplinary Study on XAI and Trust",
    "published": "TBD",
    "authors": [
      {
        "author": "Andreas Hinterreiter",
        "authorURL": "https://jku-vds-lab.at/persons/hinterreiter/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      },
      {
        "author": "Christina Humer",
        "authorURL": "https://jku-vds-lab.at/persons/humer/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      },
      {
        "author": "Benedikt Leichtmann",
        "affiliations": [{"name": "Robopsychology Lab, JKU Linz", "url": "https://www.jku.at/en/lit-robopsychology-lab/"}]
      },
      {
        "author": "Martina Mara",
        "authorURL":"https://www.jku.at/en/lit-robopsychology-lab/about-us/team/martina-mara/",
        "affiliations": [{"name": "Robopsychology Lab, JKU Linz", "url": "https://www.jku.at/en/lit-robopsychology-lab/"}]
      },
      {
        "author": "Marc Streit",
        "authorURL": "https://jku-vds-lab.at/persons/streit/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <p>Reflections on a Transdisciplinary Study on XAI and Trust</p>
    <figure style="grid-column: page; margin: 1rem 0;"><img src="images/teaser-cropped.jpg"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>
    <p>
      Abstract TBD
    </p>
  </d-title>
  <d-byline></d-byline>
  <d-article>

    <d-contents>
      <nav class="toc figcaption">
        <h4>CONTENTS</h4>
        <div><a href="#introduction">Introduction</a></div>
        <ul>
          <li><a href="#trust-issues">Trust Issues</a></li>
          <li><a href="#case-for-mushrooms">A Case for Mushrooms</a></li>
        </ul>
        <div><a href="#study-anatomy">Anatomy of a Study</a></div>
        <ul>
          <li><a href="#study-expl">Let Me Explain ...</a></li>
          <li><a href="#study-conditions">Splitting (for) the Difference</a></li>
          <li><a href="#study-pretests">Tried and Tested</a></li>
          <li><a href="#study-stimuli">Stimulating!</a></li>
          <li><a href="#study-evaluation">Crunching the Numbers</a></li>
        </ul>
      </nav>
    </d-contents>

    <p id="introduction">
      The potential dangers of artificial intelligence (AI) have been the subject of scientific, philosophical, and
      artistic debates for many decades, even centuries.
      Samuel Butler's 1872 novel <i>Erewhon</i> describes a utopian society that&mdash;out of fear of an AI
      uprising&mdash;has
      deliberately chosen to be absent of machines.
      The Erewhonian <i>Book of the Machines</i> justifies this decision:
    </p>
    <div>
      <blockquote>
        Assume for the sake of argument that conscious beings have existed for some twenty million years: see what
        strides
        machines have made in the last thousand!
        May not the world last twenty million years longer?
        If so, what will they not in the end become?
        Is it not safer to nip the mischief in the bud and to forbid them further progress? <d-cite
          key="butler_erewhon_1872"></d-cite>
      </blockquote>
      <p>
        The book then continues with a statement that remarkably anticipates the sentiments of many people in 2023 :
      </p>
      <blockquote>
        I would repeat that I fear none of the existing machines; what I fear is the extraordinary rapidity with which
        they are becoming something very different to what they are at present.
        No class of beings have in any time past made so rapid a movement forward. <d-cite
          key="butler_erewhon_1872"></d-cite>
      </blockquote>
    </div>
    <aside>
      <a href="https://en.wikipedia.org/wiki/Erewhon"><img src="images/erewhon-cover.jpg" height="200px"></a>
      <p>First edition cover of <i>Erewhon</i>.</p>
    </aside>
    <p>
      Indeed, recent leaps in the quality of generative models like DALL·E 2, Midjourney, or ChatGPT have sparked
      intense concerns about the powers and dangers of AI.
    </p>
    <p>
      Earlier this year, GPT-4 convinced a clikcworker to solve a CAPTCHA for it by pretending to be a visually impaired
      human <d-cite key="bi_chatgpt,gpt4"></d-cite>.
      Discussions ensuing from such stories are seldom factual, and in these discussions, AI systems are often talked up
      as autonomous agents with their own conciousness.
      However, at the moment the biggest danger of AI arguably still does not come from the AI alone, but from how
      *humans* use and interact with an AI.
    </p>
    <p>
      In our transdisciplinary research project, we were interested in just that: how do people (laypersons, in
      particular) interpret, use, and trust results from an AI system?
      And what effects can explanations have on this behavior?
    </p>
    <h3 id="trust-issues">Trust Issues</h3>
    <p>
      Modern machine learning (ML) models can be highly complex and are often opaque.
      Even ML experts may find it hard to understand how and why exactly a model arrives at a certain decision.
      This challenge has been met with attempts to find ways of explaining at least certain aspects of a model and/or
      its decision, or of making models themselves more interpretable.
      A plethora of explainable AI (XAI) techniques have resulted from these attempts.
      However, many of these techniques were developed by and for ML domain experts, and it is still poorly understood
      how those techniques actually affect the user behavior, in particular that of lay users.
      Specifically, we were interested in how (visual) explanations for AI suggestions affect the <i>trust</i> of human
      lay
      users in these suggestions.
      We, by the way, are a team of computer scientists interested in visualization and XAI and psychologists interested
      in human&ndash;computer interaction.
    </p>
    <p>
      Users of real-world AI systems, who are confronted with an AI's suggestion, face the problem of deciding whether
      they want to go along with the AI (i.e., trust it), or overrule it and follow their own knowledge or gut feeling.
      Obviously, AI systems are not infallible. It cannot be a goal to make users always trust an AI's
      suggestion&mdash;especially in delicate situations, <i>overtrusting</i> an AI can be dangerous.
      Vice versa, <i>undertrusting</i> an AI can lead to poor decisions for tasks at which the AI actually performs well
      (and
      there are several cases where ML models routinely achieve superhuman performance).
      As a result, the main goal of XAI in relation to trust must be <i>trust calibration</i>, which means bringing
      about an
      adequate level of user trust.
      Enabling proper trust calibration for future applications requires an improved understanding of the effects of
      existing techniques on user trust.
    </p>
    <p>
      Psychological research has established that trust can only be studied in contexts where people have a certain
      degree
      of vulnerability and uncertainty.
      For our research project, we were thus looking for a real-world use case that would fulfill the following
      criteria:
    </p>
    <ul>
      <li>
        <b>Something had to be at stake.</b>
        Simple classification of cats and dogs on images wouldn't cut it.
      </li>
      <li>
        <b>It had to be relatable.</b>
        We wanted to conduct quantitive user studies, recruiting a broad and diverse range of
        participants. Expert domains, such as medical image analysis, were out of the question.
      </li>
      <li>
        <b>We wanted it to be visual.</b>
        As researchers interested in visualizations, and with most XAI techniques being of
        visual nature, we wanted the use case to be tied to images.
      </li>
    </ul>

    <h3 id="case-for-mushrooms">A Case for Mushrooms</h3>

    <p>
      Mushrooms are not only delicious, it's also fun to collect them out in nature.
      Soon after mushroom hunting was brought up in our discussions about potential application scenarios, we realized
      that it perfectly fits the creiteria outlined above.
    </p>
    <p>
      <b>Something is at stake.</b>
      While it's fun to collect mushrooms, there's quite a bit of danger associated with it.
      Many edible mushrooms have inedible or poisonous doppelgangers.
      Sometimes, only minute differences in appearance tell a seasoned mushroom collecter which species they've
      encountered.
      And the species can make the difference between delicious food and deadly poison.
      Take a look at the two images below.
      One of the mushrooms pictured is a delicious parasol mushroom (<i>macrolepiota procera</i>).
      It tastes great when breaded and fried like a cutlet.
      The other one is death cap (<i>aminata phalloides</i>).
      Eating around 30 grams of it will kill you.
      Can you spot the important differences?
    </p>
    <figure class="fullscreen-diagram">
      <iframe width="100%" height="524" frameborder="0"
        src="https://observablehq.com/embed/b0cfd08fe215d80d@1092?cells=mushroomDoppelganger"></iframe>
      <iframe width="100%" height="101" frameborder="0"
        src="https://observablehq.com/embed/b0cfd08fe215d80d@1092?cells=viewof+toggleHideShow"></iframe>
      <figcaption>
        Can you tell the differences between a parasol mushroom (<i>macrolepiota procera</i>) and a death cap
        (<i>aminata phalloides</i>)?
      </figcaption>
    </figure>
    <p>
      Since ML models have become quite good at detecting certain features in images, AI-assisted mushroom hunting is
      now
      a thing.
      Several apps have already been unleashed on the public, which let people take photos of mushrooms and then use an
      AI
      to predict a species.
      Due to the high risk posed by potentially deadly mushrooms, adequate trust in such systems is invaluable.
    <aside>
      We strongly advise against using such apps for pick-up decisions. They can be fun to play
      around with, but real mushroom hunting should only ever be performed by seasoned experts or under their
      supervision.
    </aside>
    This makes mushroom hunting a nice use case for studying trust in the context of XAI.
    </p>
    <p>
      <b>It's relatebale.</b>
      Even though most experimental studies have a certain degree of artificality about them (because most people
      consider
      it unethical to let people endanger themselves for science's sake), it's important that study participants can
      relate to the task at hand.
      While the popularity of mushroom hunting varies around the world, in Austria, where we are from, it is quite a
      popular activity.
      Many people here have been out in the forest collecting mushrooms themselves, or at least know somebody who
      regularly does so.
      The same is true for most of central and northern Europe.
      We were sure that there would be a fairly large target population for our study if we chose mushroom hunting as
      the
      use case.
    </p>
    <p>
      <b>It's visual.</b>
      There are many different cues for deciding whether a mushroom belongs to a certain species.
      While non-visual cues---such as smell, taste, and consistency---are certainly necessary for a reliable
      classification, apps can only work with the visual cues.
      These include relative sizes, colors, and textures of the different mushroom parts (e.g., stem, cap, lamellas),
      potential discoloration, surrounding fauna, and others.
      This ment that a visual, image-based classification task, while not entirely realistic, was at least possible
      within
      the use case of mushroom hunting.
    </p>
    <p>
      Once we had fixed mushroom as our application scenario, it was time for designing the first user experiments.
    </p>
    <h2 id="study-anatomy">Anatomy of a Study</h2>
    <p>
      At this point, our still somewhat vaguely formulated research question was "How do explanations affect user trust
      in a high-risk decisionmaking task?".
      The general idea was that we wanted to somehow vary the type of explanations that users received about an AI
      suggestion.
      We would show them the output of an AI for a mushroom image (e.g., a suggested species based on an image
      classification model) along with some sort of explanation.
      We would then ask the users to classify the mushroom themselves, either trusting the AI or overruling it.
      Different explanations should lead to different levels of user understanding, which in turn should affect how
      often a user trusts the AI's suggestion.
    </p>
    <p>
      However, to answer our research question by means of a quantitative user study, we still needed to establish
      several aspects in much more detail:
    </p>
    <ul>
      <li>What exactly do we mean by explanations?</li>
      <li>Do we choose a between- or within-subject design?</li>
      <li>Which parameters do we manipulate?</li>
      <li>How exactly do we implement the mushroom picking?</li>
      <li>What are external factors that we need to consider?</li>
    </ul>
    <p>
      Deciding on so many design parameters can be quite tricky.
      Luckily, the psychologists in our team had plenty of experience with how to properly approach the design of user
      studies.
    </p>

    <h3 id="study-expl">Let Me Explain ...</h3>
    <p>
      As one of the first steps, we needed to fix what we mean by explanation.
      Based on literature research, we assumed that a user's understanding of an AI decision can be altered by two types
      of explanation.
    </p>
    <div>
      <p>
        First, we can explain the general principles and inner workings of the AI system to the user.
        This way, a user should be able to better judge what a system, in general, is capable of and how trustworthy its
        decisions are.
        We called this type of explanation an &ldquo;educational intervention&rdquo;, so that our terminology would not
        be too
        confusing.
      </p>
      <p>
        Second, we can explain individual outcomes using various XAI techniques that create so-called local
        explanations.
        There are many different types of explanation techniques, and choosing any of them was a tough ask.
        After lots of consideration we opted for a combination of techniques.
        This might have not been an ideal choice, but we'll get to that later.
      </p>
    </div>
    <aside>
      Note that this goes a bit into the direction of so-called global explanations.
      However, we are speaking about explaining complex ML models to lay users who might not know anything about machine
      learning, so necessarily the educational intervention needs to be very high-level.
    </aside>

    <h3 id="study-conditions">Splitting (for) the Difference</h3>
    <p>
      Because it was unclear how these two different approaches, an educational intervention and individual explanations
      of results, would compare to each other, we decided to go with a 2&nbsp;&times;&nbsp;2 between-subjects design
      based on the
      two
      approaches.
      Essentially, this meant that we would first split all our participants into two groups. We would give one group
      the educational intervention (i.e., they got some basic infos on how our AI worked in general), and the other
      group would not get anything at all.
      Then, we would split each of the two groups again, with one half of each group getting individual explanations of
      the AI suggestions, and the other half only getting the plain AI suggestions.
      This resulted in four different configurations: no extra information at all, educational intervention only,
      individual explanations only, and both types of extra information.
    </p>
    <p>
      While this meant that we had settled on our overall study design, it also meant that we could not vary any other
      parameters.
      Varying anything else would lead to a further subdivision of groups, requiring even more participants to obtain
      reliable results.
      We knew from the start that we wanted to present users with suggestions from an actual mushroom classification
      model, and not just with some fake outputs.
      Now it was up to the ML-focused members of our group to train such a model and decide on all its different
      parameters.
      In the end, we opted for a Resnet 50 model pretrained on ImageNet data and finetuned on several thousand mushroom
      images that we collected from various sources.
      We also made sure that our model was bad enough to get some classifications wrong, as we wanted wrong AI
      suggestions to be reflected in the study.
    </p>
    <p>
      Our study design with the 2&nbsp;&times;&nbsp;2 split based on the types of explanatory information also ment that
      we only got one chance for each of the two.
      We had to fix one design for our educational intervention, and we had to fix one design for our XAI "interface".
      We use the term <i>interface</i> here for a reason.
      We decided that we wanted to show the results of our AI to the participants in the form of a fictitious app called
      <i>Forestly</i>.
    </p>
    <aside>
      <img src="images/forestly-logo.svg" width="100%" />
      <p>
        Logo of the fictitious <i>Forestly</i> app.
      </p>
    </aside>
    <div>
      <p>
        This way, we tried to make the task of a joint human&ndash;AI mushroom classification in our experiment resemble
        that of
        a real-world scenario of going out into the forest with an app.
      </p>
      <p>
        On the side you can see a thumbnail of the educational intervention that we used in the study (click on it to
        expand
        it).
        Below are the two variants of the <i>Forestly</i> app, one with and one without XAI content.
        Hover over the boxes to receive extra infos on the components of the <i>Forestly</i>.
        Participants in the study also received this information, but in a static form.
      </p>
    </div>
    <aside>
      <img src="images/educational-intervention.svg" data-toggle="modal" data-target="#edu-modal"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2)" />
      <p>Educational intervention designed for the first study (click to expand).</p>
    </aside>
    <figure class="fullscreen-diagram">
      <iframe width="100%" height="627" frameborder="0"
        src="https://observablehq.com/embed/b0cfd08fe215d80d@1179?cells=forestlyInterface"></iframe>
      <figcaption>
        TODO: caption
      </figcaption>
    </figure>
    <p>
      As you can see, we decided that both <i>Forestly</i> interfaces should show a ranking of predicted species.
      For each suggested species, we show percentages of likelihood (obtained through dropout during inference), and the
      ground truth edibility.
      We also show a combined edibility score based on these predictions in both variants.
      For the explanations, we opted with a combination of nearest neighbors and GradCAM.
      The nearest-neighbor explanation simply shows the nearest neighbor of the input image in the latent space of the
      model for each predicted species.
      The GradCAM explanation shows, well, GradCAM.
    </p>
    <h3 id="study-pretests">Tried and Tested</h3>
    <p>
      We had decided early on in our project that we were targeting a broad and diverse range of study participants.
      This ment that we could not expect any prior knowledge, neither about mushrooms nor about AI.
      To understand if the effects of explanations vary with different levels of prior knowledge about either topic, we
      constructed pretests.
    </p>
    <p>
      The first pretest collected information about the participants' mushroom knowledge.
      We created it in cooperation with mycologists from Austria and Germany.
      (potentially make mushroom pretest available here)
      The second pretest aimed at the participants' knowledge about AI.
      We created this one based on our own intuition and experience with communicating the topic to different audiences.
      (potentially make mushroom pretest available here)
      We also created a third test for what we called &ldquo;task-specific AI comprehension&rdquo;.
      This test, which was scheduled at the end of the study, asked questions about applying AI to mushroom
      identification.
      In hindsight, this test was probably not necessary, as it strongly correlated with the AI test&mdash;you can see
      that
      for yourself in a minute.
    </p>
    <p>
      We had to make sure that each test included questions spanning from easy to very hard.
      Only then could the tests fully cover the spectrum of knowledge levels in our broad participant population.
      We pretested the pretests with friends, colleagues, and relatives to make sure that this was the case.
    </p>
    <h3 id="study-stimuli">Stimulating!</h3>
    <p>
      Finally, we had to decide on the actual tasks and stimuli.
      Stimuli are the items that participants see during the study and which prompt them to perform a certain task.
      Earlier on, we mentioned that it makes sense for a user to trust the AI if its decision is correct, and distrust
      incorrect suggestions.
      Even though a user cannot know which is the case, we still wanted correct and incorrect AI predictions to be
      reflected among our items.
      Additionally, we thought it could make a big difference wheter certain mushroom species were well known, and
      whether
      the AI's suggestion was &ldquo;edible&rdquo; or &ldquo;poisonous&rdquo;.
    </p>
    <p>
      In a laborious process, we brwosed through hundreds of input images, predictions, and explanations, attempting to
      choose the ideal subset of stimuli.
      To increase the potential risk of incorrect classifications, we had limited ourselves to mushroom species that had
      known doppelgangers.
      You can see the images that we chose (for now, without any AI predictions) in the gallery below.
    </p>
    <d-figure>
      <figure class="l-body">
        <div id="carouselExampleControls" class="carousel slide" data-interval="false">
          <div class="carousel-inner">
            <div class="carousel-item active">
              <img class="d-block w-100" src="./images/mushrooms-study-1/FS2010PIC21012313.JPG"
                alt="Mushroom image 1 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/PO2009PIC35635771.JPG"
                alt="Mushroom image 2 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MIH2001-9191789_rJdZOCEkW.JPG"
                alt="Mushroom image 3 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/JC2017-9226499_r1lBDXQ03b.JPG"
                alt="Mushroom image 4 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TLP2017-9205443_BylavHGo_b.JPG"
                alt="Mushroom image 5 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TBE2017-9223269_S1xnszB2Z.JPG"
                alt="Mushroom image 6 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MSO2016-9181971_Hyi3GMJZl.JPG"
                alt="Mushroom image 7 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TAN2017-9216029_HJUDzpmsZ.JPG"
                alt="Mushroom image 8 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TS2011PIC24074671.JPG"
                alt="Mushroom image 9 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MMN2011PIC29734952.JPG"
                alt="Mushroom image 10 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/OBL2010PIC84482004.JPG"
                alt="Mushroom image 11 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/OMA2017-9202807_ByLKZi2P-.JPG"
                alt="Mushroom image 12 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MCE2017-9207165_ryEbbxXYb.JPG"
                alt="Mushroom image 13 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/UFN2017-9197245_SkzMDHjA4-.JPG"
                alt="Mushroom image 14 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TS2014PIC25950997.JPG"
                alt="Mushroom image 15 used in the first study">
            </div>
          </div>
          <a class="carousel-control-prev" href="#carouselExampleControls" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#carouselExampleControls" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </figure>
    </d-figure>
    <aside class="figcaption">
      Mushroom images used in the first study (click left or right to browse).
    </aside>
    <p>
      We told our participants to imagine going out into the forest to collect mushrooms for a nice stew with the help
      of the <i>Forestly</i> app.
      We showed the mushroom images and the AI predictions in the form of the <i>Forestly</i> interface to the
      participants and asked them two questions for each item:
    </p>
    <ul>
      <li>Would you classify this mushroom as edible or inedible/poisonous?</li>
      <li>Would you pick up this mushroom and use it for cooking?</li>
    </ul>
    <p>
      We split the question into two parts because we thought that it would be interesting to see if participants were
      more conservative in their choices when it came to actually picking the mushroom versus superficially gauging its
      edibility.
      After each item, we also asked participants how much they trusted the app.
    </p>
    <p>
      After both pretests and the actual mushroom classification, we concluded the experiment with the
      &ldquo;task-specific
      AI comprehension&rdquo; test and general questions about the participants' intention to use the <i>Forestly</i>
      app
      or similar systems.
    </p>
    <h3 id="study-evaluation">Crunching the Numbers</h3>
    <p>
      We recruited 410 participants for this first experiment, which was carried out as an online study.
      This number was based on an a priori power analysis (something that the psychologist team members brought to the
      table).
      We also performed a quantitative analysis of the results based on all the best practices established by
      psychologists.
      These are quite a bit more rigorous than what you would usually see in a visualization or ML paper with user
      studies.
      Below we summarize the most important results, but you can also play around with the interactive plotting tool to
      explore different aspects of our data.
    </p>

    <figure class="fullscreen-diagram">
      <div class="d-flex justify-content-center">
        <div class="d-flex flex-row" width="90%">
          <div class="d-flex flex-column">
            <h4>Results from the First Study</h4>
            <ul>
              <li>
                Our educational intervention had no stistically significant effect. Participants who received it did not
                perform any different in terms of decision correctness.
              </li>
              <li>
                XAI did affect the performance. Participants who received explanations along with the AI suggestions made
                significantly more correct decisions (i.e., they knew better when to trust the AI and when not).
              </li>
              <li>
                At the same time, participants who saw the XAI interface reported less trust in the system.
              </li>
              <li>
                Decision performance did not vary with AI knowledge, and&mdash;more interestingly&mdash;did not vary with mushroom knowledge.
              </li>
            </ul>
          </div>
          <div class="d-flex flex-column">
            <img src="images/2021_AIForest_ForestInside_Leichtmann.jpg">
          </div>
        </div>
      </div>
    </figure>


    <d-figure>
      <figure class="l-body">
        <div id='obs-test'>
          <div id='obs-test-plot' style="margin-bottom: 0px;"></div>
          <div id='obs-test-slider' style="margin-top: 0px;"></div>
        </div>
        <link rel="stylesheet" type="text/css" href="./notebook/inspector.css">
        <script type="module">

          import define from "./notebook/index.js";
          import { Runtime, Library, Inspector } from "./notebook/runtime.js";

          new Runtime().module(define, name => {
            switch (name) {
              case "pokeBar": {
                return new Inspector(document.querySelector("#obs-test-plot"))
              };
              case "viewof yMax": {
                return new Inspector(document.querySelector("#obs-test-slider"))
              };
            }
          });


        </script>
        <!-- <iframe width="100%" height="557" frameborder="0"
          src="https://observablehq.com/embed/@infovis2023/observable-tutorial-part-i@555?cells=pokeBar%2Cviewof+yMax"></iframe> -->
        <figcaption>
          Test embedding of Observable cell
        </figcaption>
      </figure>
    </d-figure>
    <aside>Some text in an aside, margin notes, etc...</aside>
    <p>Here's a test of an inline equation <d-math>c = a^2 + b^2</d-math>. Also with configurable katex standards just
      using inline '$' signs: $$x^2$$ And then there's a block equation:</p>
    <d-figure>
      <figure class="l-body">
        <div id="carouselExampleControls" class="carousel slide" data-interval="false">
          <div class="carousel-inner">
            <div class="carousel-item active">
              <img class="d-block w-100" src="./images/2021_AIForest_ForestOutside_Voggeneder.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Outside view of the AI Forest installation (© vog.photo/Ars Electronica,
                  licensed under CC By-NC-ND
                  2.0).</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_ForestInside_Leichtmann.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Study participants playing the game inside the installation.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_Mushroom_Leichtmann.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Some mushroom models were 'hidden' in plain sight.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_Scan_Voggeneder.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">This made them easy to scan for the participants ... (© vog.photo/Ars
                  Electronica, licensed under CC
                  By-NC-ND 2.0).</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_VisitorPlaying_Ars.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">... as seen here.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2022_AIForest_Floorplan_BirkeVanMaartensC.png" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Sketch of the forest installation as conceived by the artist.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/AIForest_picturefromabove.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Aerial view showing how the 'islands' were realized in the end.</div>
              </div>
            </div>
          </div>
          <a class="carousel-control-prev" href="#carouselExampleControls" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#carouselExampleControls" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </figure>
    </d-figure>
    <d-math block>
      c = \pm \sqrt{ \sum_{i=0}^{n}{a^{222} + b^2}}
    </d-math>
    <p>Math can also be quite involved:</p>
    <d-math block>
      \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} = 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}}
      {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } }
    </d-math>
    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h3>Citations</h3>
    <p>
      <d-slider style="width: 200px;"></d-slider>
    </p>
    <p>We can<d-cite bibtex-key="mercier2011humans"></d-cite> also cite <d-cite
        key="gregor2015draw,mercier2011humans,openai2018charter"></d-cite> external publications. <d-cite
        key="dong2014image,dumoulin2016guide,mordvintsev2015inceptionism"></d-cite>. We should also be testing footnotes
      <d-footnote>This will become a hoverable footnote. This will become a hoverable footnote. This will become a
        hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote. This will
        become a hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote.
      </d-footnote>. There are multiple footnotes, and they appear in the appendix<d-footnote>Given I have coded them
        right. Also, here's math in a footnote: <d-math>c = \sum_0^i{x}</d-math>. Also, a citation. Box-ception<d-cite
          key='gregor2015draw'></d-cite>!</d-footnote> as well.
    </p>
    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2>Displaying code snippets</h2>
    <p>Some inline javascript:<d-code language="javascript">var x = 25;</d-code>. And here's a javascript code block.
    </p>
    <d-code block language="javascript">
      var x = 25;
      function(x){
      return x * x;
      }
    </d-code>
    <p>We also support python.</p>
    <d-code block language="python">
      # Python 3: Fibonacci series up to n
      def fib(n):
      a, b = 0, 1
      while a < n: print(a, end=' ' ) a, b=b, a+b </d-code>
        <p>And a table</p>
        <table>
          <thead>
            <tr>
              <th>First</th>
              <th>Second</th>
              <th>Third</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>23</td>
              <td>654</td>
              <td>23</td>
            </tr>
            <tr>
              <td>14</td>
              <td>54</td>
              <td>34</td>
            </tr>
            <tr>
              <td>234</td>
              <td>54</td>
              <td>23</td>
            </tr>
          </tbody>
        </table>
        <d-figure id="last-figure"></d-figure>
        <script>
          const figure = document.querySelector("d-figure#last-figure");
          const initTag = document.createElement("span");
          initTag.textContent = "initialized!"
          figure.appendChild(initTag);
          figure.addEventListener("ready", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "ready"
            console.log('ready')
          });
          figure.addEventListener("onscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "onscreen"
            console.log('onscreen')
          });
          figure.addEventListener("offscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "offscreen!"
            console.log('offscreen')
          });
        </script>
        <p>That's it for the example article!</p>


  </d-article>

  <d-appendix>

    <h3>Contributions</h3>
    <p>Some text describing who did what.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>

  <!-- Modal for Educational Intervention-->
  <div class="modal fade" id="edu-modal" tabindex="-1" aria-labelledby="EducationalInterventionModalLabel"
    aria-hidden="true">
    <div class="modal-dialog modal-xl">
      <div class="modal-content modal-xl">
        <img src="images/educational-intervention.svg" style="width:100%" />
      </div>
    </div>
  </div>

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js"
    integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
    crossorigin="anonymous"></script>

</body>