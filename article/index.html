<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css" href="./notebook/inspector.css">
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
  <style>
    /* own stuff */

    figure table img {
      width: auto;
    }

    .legend-ramp {
      width: 62.5%;
      background: none;
    }

    .card-text {
      font-size: 16px;
      line-height: 24px;
    }

    .inline-icon {
      width: 2ex;
      height: 2ex;
      margin-left: 0.16em;
      margin-right: 0.16em;
    }

    .side-caption {
      margin-top: 1ex;
    }

    .subcaption {
      margin-top: 1.5ex;
    }

    @media (max-width: 1100px) {
      .fullscreen {
        margin-left: auto;
        margin-right: auto;
        grid-column: screen;
        width: 100%;
      }
    }

    @media (min-width: 1100px) {
      .fullscreen {
        margin-left: auto;
        margin-right: auto;
        grid-column: screen;
        width: 85%;
        max-width: 2000px;
      }
    }

    /* bootstrap modal */

    .modal {
      text-align: center;
      padding: 0 !important;
    }

    .modal:before {
      content: '';
      display: inline-block;
      height: 100%;
      vertical-align: middle;
      margin-right: -4px;
    }

    .modal-dialog {
      display: inline-block;
      text-align: left;
      vertical-align: middle;
    }

    @media (min-width: 992px) {
      .modal-xl {
        max-width: 1400px;
      }
    }

    /* table of contents */

    @media (max-width: 1000px) {
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1000px) {
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }
  </style>

  <style id="distill-article-specific-styles">
    @media (min-height: 900px) {
      d-article hr {
        margin-top: 120px !important;
        margin-bottom: 100px !important;
      }
    }

    .subgrid {
      grid-column: screen;
      display: grid;
      grid-template-columns: inherit;
      grid-template-rows: inherit;
      grid-column-gap: inherit;
      grid-row-gap: inherit;
    }

    d-figure.base-grid {
      grid-column: screen;
      background: hsl(0, 0%, 97%);
      padding: 20px 0;
      border-top: 1px solid rgba(0, 0, 0, 0.1);
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    }

    d-figure {
      margin-bottom: 1em;
      position: relative;
    }

    d-figure>figure {
      margin-top: 0;
      margin-bottom: 0;
    }

    .shaded-figure {
      background-color: hsl(0, 0%, 97%);
      border-top: 1px solid hsla(0, 0%, 0%, 0.1);
      border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
      padding: 30px 0;
    }

    .pointer {
      position: absolute;
      width: 26px;
      height: 26px;
      top: 26px;
      left: -48px;
    }

    .fullscreen-diagram {
      grid-column: screen;
      background: #f8f8fb;
      padding: 10px;
      padding-top: 40px;
      padding-bottom: 40px;
      border-top: 1px solid #f0f0f0;
      border-bottom: 1px solid #f0f0f0;
      margin-top: 40px;
      margin-bottom: 60px;
    }

    .margin-diagram {
      grid-column: 12 / 15;
      grid-row: auto / span 3;
      margin-top: 6px;
    }

    @media (min-width: 1800px) {
      .margin-diagram {
        margin-left: 60px;
      }
    }

    d-footnote {
      margin-left: -1px;
      margin-right: 4px;
    }

    .footnote-sep {
      width: 0px;
    }

    .footnote-sep::before {
      line-height: 1em;
      top: -0.5em;
      font-size: 0.75em;
      color: #999;
      margin-left: -6px;
      margin-right: 1px;
      content: ',';
    }

    d-footnote[comma]::before {
      vertical-align: super;
      line-height: 1em;
      top: -0.5em;
      font-size: 0.75em;
      color: #999;
      margin-left: -6px;
      margin-right: 1px;
      content: ',';
    }

    d-article h2 {
      border-bottom: none;
    }

    /* .fullscreen {
      margin-left: auto;
      margin-right: auto;
      grid-column: screen;
      width: 100%;
      max-width: 2000px;
    } */

    /* ****************************************
     * TOC
     ******************************************/
    @media (max-width: 1300px) {
      d-contents {
        display: none;
      }
    }

    b i {
      display: inline;
    }

    d-article d-contents {
      align-self: start;
      grid-column: 1 / 4;
      grid-row: auto / span 4;
      justify-self: end;
      margin-top: 0em;
      padding-right: 3em;
      padding-left: 2em;
      border-right: 1px solid rgba(0, 0, 0, 0.1);
    }

    d-contents a:hover {
      border-bottom: none;
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav div {
      color: rgba(0, 0, 0, 0.8);
      font-weight: bold;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }

    d-contents nav ul li {
      margin-bottom: 0.25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }

    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
      text-decoration: none;
    }

    d-article h2 {
      border-bottom: none !important;
    }

    d-article h3 {
      font-size: 26px !important;
    }
  </style>
  <style>
    .carousel .carousel-item {
      height: 550px;
    }

    .carousel-item img {
      position: absolute;
      object-fit: contain;
      top: 0;
      left: 0;
      max-height: 515px;
    }

    .carousel-control-prev,
    .carousel-control-next {
      border-bottom: 0px;
      opacity: 0.5;
    }

    .carousel-control-prev:hover,
    .carousel-control-next:hover {
      border-bottom: 0px;
      opacity: .9;
    }

    .carousel-caption {
      color: black;
      position: relative;
      left: auto;
      right: auto;
      top: 500px;
    }

    .carousel-control-prev-icon {
      background-image: url("data:image/svg+xml;charset=utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='%23c7c7c7' viewBox='0 0 8 8'%3E%3Cpath d='M5.25 0l-4 4 4 4 1.5-1.5-2.5-2.5 2.5-2.5-1.5-1.5z'/%3E%3C/svg%3E");
    }

    .carousel-control-next-icon {
      background-image: url("data:image/svg+xml;charset=utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='%23c7c7c7' viewBox='0 0 8 8'%3E%3Cpath d='M2.75 0l-1.5 1.5 2.5 2.5-2.5 2.5 1.5 1.5 4-4-4-4z'/%3E%3C/svg%3E");
    }
  </style>
</head>

<body>
  <distill-header></distill-header>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Of Deadly Skullcaps and Amethyst Deceivers",
    "description": "Reflections on a Transdisciplinary Study on XAI and Trust",
    "published": "TBD",
    "authors": [
      {
        "author": "Andreas Hinterreiter",
        "authorURL": "https://jku-vds-lab.at/persons/hinterreiter/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      },
      {
        "author": "Christina Humer",
        "authorURL": "https://jku-vds-lab.at/persons/humer/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      },
      {
        "author": "Benedikt Leichtmann",
        "affiliations": [{"name": "Robopsychology Lab, JKU Linz", "url": "https://www.jku.at/en/lit-robopsychology-lab/"}]
      },
      {
        "author": "Martina Mara",
        "authorURL":"https://www.jku.at/en/lit-robopsychology-lab/about-us/team/martina-mara/",
        "affiliations": [{"name": "Robopsychology Lab, JKU Linz", "url": "https://www.jku.at/en/lit-robopsychology-lab/"}]
      },
      {
        "author": "Marc Streit",
        "authorURL": "https://jku-vds-lab.at/persons/streit/",
        "affiliations": [{"name": "Visual Data Science Lab, JKU Linz", "url": "https://jku-vds-lab.at/"}]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <p>Reflections on a Transdisciplinary Study on XAI and Trust</p>
    <figure class="d-flex justify-content-around" style="grid-column: page; margin: 1rem 0;">
      <img src="images/hoxai-teaser-1.jpg" style="width:22%;" />
      <img src="images/hoxai-teaser-2.jpg" style="width:22%;" />
      <img src="images/hoxai-teaser-3.jpg" style="width:22%;" />
      <img src="images/hoxai-teaser-4.jpg" style="width:22%;" />
    </figure>
    <p>
      Does explainability change how users interact with an artificially intelligent agent?
      We sought to answer this question in a transdisciplinary research project with a team of computer scientists and
      psychologists.
      We chose the high-risk decisionmaking task of AI-assisted mushroom hunting to study the effects that explanations
      of AI suggestions have on user trust.
      We present the summarized results of three studies, one of which was carried out in an unusual environment as part
      of an art festival.
      Our results show that visual explanations can lead to more adequate trust in AI systems and thereby to an improved
      decision correctness.
    </p>
  </d-title>
  <d-byline></d-byline>
  <d-article>

    <d-contents>
      <nav class="toc figcaption">
        <h4>Contents</h4>
        <div><a href="#introduction">Introduction</a></div>
        <ul>
          <li><a href="#trust-issues">Trust Issues</a></li>
          <li><a href="#case-for-mushrooms">A Case for Mushrooms</a></li>
        </ul>
        <div><a href="#study-anatomy">Anatomy of a Study</a></div>
        <ul>
          <li><a href="#study-expl">Let Me Explain ...</a></li>
          <li><a href="#study-conditions">Splitting (for) the Difference</a></li>
          <li><a href="#study-pretests">Tried and Tested</a></li>
          <li><a href="#study-stimuli">Stimulating!</a></li>
          <li><a href="#study-evaluation">Crunching the Numbers</a></li>
        </ul>
        <div><a href="#ars-study">Going Artistic</a></div>
        <ul>
          <li><a href="#ars-game">I Want to Play a Game</a></li>
          <li><a href="#ars-forest">Wood Chips & Fragrance Dispensers</a></li>
        </ul>
        <div><a href="study-3">What's the Difference?</a></div>
        <ul>
          <li><a href="study-3-dissect">A New Challenger Has Appeared</a></li>
          <li><a href="study-3-align">Something's Awry</a></li>
        </ul>
        <div><a href="#blaming">Who's to Blame?</a></div>
        <div><a href="#conclusion">Conclusion</a></div>
      </nav>
    </d-contents>
    <div class="card text-white" style="width: 100%; background-color: hsl(200, 60%, 15%); margin-bottom: 2ex;">
      <img class="card-img-top" src="images/pexels-photo-2952871-cropped.jpg" alt="Book shelf">
      <div class="card-body">
        <h4 class="card-title">Citation information</h4>
        <p class="card-text">
          This article contains results from previously published work and from unpublished preprints.
          Cards like this one will guide you to the original sources for each section.</p>
      </div>
    </div>
    <p id="introduction">
      The potential dangers of artificial intelligence (AI) have been the subject of scientific, philosophical, and
      artistic debates for many decades, even centuries.
      Samuel Butler's 1872 novel <em>Erewhon</em> describes a utopian society that&mdash;out of fear of an AI
      uprising&mdash;has
      deliberately chosen to be absent of machines.
      The Erewhonian <em>Book of the Machines</em> justifies this decision:
    </p>
    <div>
      <blockquote>
        Assume for the sake of argument that conscious beings have existed for some twenty million years: see what
        strides
        machines have made in the last thousand!
        May not the world last twenty million years longer?
        If so, what will they not in the end become?
        Is it not safer to nip the mischief in the bud and to forbid them further progress? <d-cite
          key="butler_erewhon_1872"></d-cite>
      </blockquote>
      <p>
        The book then continues with a statement that remarkably anticipates the sentiments of many people in 2023 :
      </p>
      <blockquote>
        I would repeat that I fear none of the existing machines; what I fear is the extraordinary rapidity with which
        they are becoming something very different to what they are at present.
        No class of beings have in any time past made so rapid a movement forward. <d-cite
          key="butler_erewhon_1872"></d-cite>
      </blockquote>
    </div>
    <aside>
      <a href="https://en.wikipedia.org/wiki/Erewhon"><img src="images/erewhon-cover.jpg" height="200px"></a>
      <p class="side-caption">First edition cover of <em>Erewhon</em>.</p>
    </aside>
    <p>
      Indeed, recent leaps in the quality of generative models like DALL·E 2, Midjourney, or ChatGPT have sparked
      intense concerns about the powers and dangers of AI.
    </p>
    <p>
      Earlier this year, GPT-4 convinced a clickworker to solve a CAPTCHA for it by pretending to be a visually impaired
      human <d-cite key="bi_chatgpt,gpt4"></d-cite>.
      Discussions ensuing from such stories are seldom factual, and in these discussions, AI systems are often talked up
      as autonomous agents with their own consciousness.
      However, at the moment the biggest danger of AI arguably still does not come from the AI alone, but from how
      <em>humans</em> use and interact with an AI.
    </p>
    <p>
      In our transdisciplinary research project, we were interested in just that: how do people (laypersons, in
      particular) interpret, use, and trust results from an AI system?
      And what effects can explanations have on this behavior?
    </p>
    <h3 id="trust-issues">Trust Issues</h3>
    <p>
      Modern machine learning (ML) models can be highly complex and are often opaque.
      Even ML experts may find it hard to understand how and why exactly a model arrives at a certain decision.
      This challenge has been met with attempts to find ways of explaining at least certain aspects of a model and/or
      its decision, or of making models themselves more interpretable.
      A plethora of explainable AI (XAI) techniques have resulted from these attempts <d-cite
        key="molnar_interpretable_2022"></d-cite>.
      However, many of these techniques were developed by and for ML domain experts, and it is still poorly understood
      how those techniques actually affect the user behavior, in particular that of lay users.
      Specifically, we were interested in how (visual) explanations for AI suggestions affect the <em>trust</em> of
      human lay users in these suggestions.
      We, by the way, are a team of psychologists interested in human&ndash;computer interaction and computer scientists
      interested in visualization and XAI.
    </p>
    <p>
      Users of real-world AI systems, who are confronted with an AI's suggestion, face the problem of deciding whether
      they want to go along with the AI (i.e., trust it), or overrule it and follow their own knowledge or gut feeling.
      Obviously, AI systems are not infallible. It cannot be a goal to make users always trust an AI's
      suggestion&mdash;especially in delicate situations, <em>overtrusting</em> an AI can be dangerous.
      Vice versa, <em>undertrusting</em> an AI can lead to poor decisions for tasks at which the AI actually performs
      well (and there are more and more applications where AI can achieve superhuman performance <d-cite
        key="brown_superhuman_2019,mirhoseini_graph_2021"></d-cite>).
      As a result, the main goal of XAI in relation to trust must be <em>trust calibration</em>, which means bringing
      about an adequate level of user trust.
      Enabling proper trust calibration for future applications requires an improved understanding of the effects
      of existing techniques on user trust.
    </p>
    <p>
      Psychological research has established that trust can only be studied in contexts where people have a certain
      degree of vulnerability and uncertainty <d-cite key="lee_trust_2004,hannibal_robot_2021"></d-cite>.
      For our research project, we were thus looking for a real-world use case that would fulfill the following
      criteria:
    </p>
    <ul>
      <li>
        <b>Something had to be at stake.</b>
        Simple classification of cats and dogs on images wouldn't cut it.
      </li>
      <li>
        <b>It had to be relatable.</b>
        We wanted to conduct quantitative user studies, recruiting a broad and diverse range of
        participants. Expert domains, such as medical image analysis, were out of the question.
      </li>
      <li>
        <b>We wanted it to be visual.</b>
        As researchers interested in visualizations, and with most XAI techniques being of
        visual nature, we wanted the use case to be tied to images.
      </li>
    </ul>

    <h3 id="case-for-mushrooms">A Case for Mushrooms</h3>

    <p>
      Mushrooms are not only delicious, it's also fun to collect them out in nature.
      Soon after mushroom hunting was brought up in our discussions about potential application scenarios, we realized
      that it perfectly fits the criteria outlined above.
    </p>
    <p>
      <b>Something is at stake.</b>
      While it's fun to collect mushrooms, there's quite a bit of danger associated with it.
      Many edible mushrooms have inedible or poisonous doppelgangers.
      Sometimes, only minute differences in appearance tell a seasoned mushroom collector which species they've
      encountered.
      And the species can make the difference between delicious food and deadly poison.
      Take a look at the two images below.
      One of the mushrooms pictured is a delicious parasol mushroom (<em>macrolepiota procera</em>).
      It tastes great when breaded and fried like a cutlet.
      The other one is death cap (<em>aminata phalloides</em>).
      Eating around 30 grams of it will kill you.
      Can you spot the important differences?
    </p>
    <div class="figcaption" id='doppelganger-checkbox'></div>
    <figure class="fullscreen" style="margin-top: 0px;">
      <div class="d-flex justify-content-center" id='doppelganger-main' style="margin: 0 auto; width: 1200px;"></div>
      <div class="d-flex justify-content-center">
        <figcaption>
          One of these mushrooms is a delicious parasol, the other one a poisonous death cap. Can you spot the
          differences?
        </figcaption>
      </div>
    </figure>
    <p>
      Since ML models have become quite good at detecting certain features in images, AI-assisted mushroom hunting is
      now a thing.
      Several apps have already been unleashed on the public, which let people take photos of mushrooms and then use an
      AI to predict a species.
      Due to the high risk posed by potentially deadly mushrooms, adequate trust in such systems is invaluable.
    </p>
    <aside>
      We strongly advise against using such apps for pick-up decisions. They can be fun to play
      around with, but real mushroom hunting should only ever be performed by seasoned experts or under their
      supervision.
    </aside>
    <p>
      This makes mushroom hunting a nice use case for studying trust in the context of XAI.
    </p>
    <p>
      <b>It's relatable.</b>
      Even though most experimental studies have a certain degree of artificiality about them (because most people
      consider
      it unethical to let people endanger themselves for science's sake), it's important that study participants can
      relate to the task at hand.
      While the popularity of mushroom hunting varies around the world, in Austria, where we are from, it is quite a
      popular activity.
      Many people here have been out in the forest collecting mushrooms themselves, or at least know somebody who
      regularly does so <d-cite key="aigner_ethnomykologische_2016"></d-cite>.
      The same is true for most of central and northern Europe <d-cite
        key="svanberg_mushroom_2019,kaaronen_mycological_2020"></d-cite>.
      We were sure that there would be a fairly large target population for our study if we chose mushroom hunting as
      the use case.
    </p>
    <p>
      <b>It's visual.</b>
      There are many different cues for deciding whether a mushroom belongs to a certain species.
      While non-visual cues---such as smell, taste, and consistency---are certainly necessary for a reliable
      classification, apps can only work with the visual cues.
      These include relative sizes, colors, and textures of the different mushroom parts (e.g., stem, cap, lamellas),
      potential discoloration, surrounding fauna, and others.
      This meant that a visual, image-based classification task, while not entirely realistic, was at least possible
      within the use case of mushroom hunting.
    </p>
    <p>
      Once we had fixed mushroom as our application scenario, it was time for designing the first user experiments.
    </p>
    <h2 id="study-anatomy">Anatomy of a Study</h2>
    <div class="card text-white" style="width: 100%; background-color: hsl(200, 60%, 15%); margin-bottom: 2ex;">
      <img class="card-img-top" src="images/pexels-photo-2952871-cropped.jpg" alt="Book shelf">
      <div class="card-body">
        <a href="https://doi.org/10.1016/j.chb.2022.107539" class="stretched-link"></a>
        <h4 class="card-title">
          Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task
        </h4>
        <p class="card-text">
          Benedikt Leichtmann, Christina Humer, Andreas Hinterreiter, Marc Streit, Martina Mara.
        </p>
        <p class="card-text">
          Computers in Human Behavior, 139: 107539, DOI: 10.1016/j.chb.2022.107539, 2023.
        </p>
      </div>
    </div>
    <p>
      At this point, our still somewhat vaguely formulated research question was "How do explanations affect user trust
      in a high-risk decisionmaking task?".
      The general idea was that we wanted to somehow vary the type of explanations that users received about an AI
      suggestion.
      We would show them the output of an AI for a mushroom image (e.g., a suggested species based on an image
      classification model) along with some sort of explanation.
      We would then ask the users to classify the mushroom themselves, either trusting the AI or overruling it.
      Different explanations should lead to different levels of user understanding, which in turn should affect how
      often a user trusts the AI's suggestion.
    </p>
    <p>
      However, to answer our research question by means of a quantitative user study, we still needed to establish
      several aspects in much more detail:
    </p>
    <ul>
      <li>What exactly do we mean by explanations?</li>
      <li>Do we choose a between- or within-subject design?</li>
      <li>Which parameters do we manipulate?</li>
      <li>How exactly do we implement the mushroom picking task?</li>
      <li>What are external factors that we need to consider?</li>
    </ul>
    <p>
      Deciding on so many design parameters can be quite tricky.
      Luckily, the psychologists in our team had plenty of experience with how to properly approach the design of user
      studies.
    </p>

    <h3 id="study-expl">Let Me Explain ...</h3>
    <p>
      As one of the first steps, we needed to fix what we mean by explanation.
      Based on literature research, we assumed that a user's understanding of an AI decision can be altered by two types
      of explanation.
    </p>
    <div>
      <p>
        First, we can explain the general principles and inner workings of the AI system to the user.
        This way, a user should be able to better judge what a system, in general, is capable of and how trustworthy its
        decisions are <d-cite key="ng_ai_2021,long_what_2020"></d-cite>.
        We called this type of explanation an &ldquo;educational intervention&rdquo;, so that our terminology would not
        be too confusing.
      </p>
      <p>
        Second, individual outcomes can be explained using XAI methods <d-cite
          key="miller_explanation_2019,long_what_2020"></d-cite>.
        There are many different types of explanation techniques, and choosing any of them was a tough ask.
        After lots of consideration we opted for a combination of techniques.
        This might have not been an ideal choice, but we'll get to that later.
      </p>
    </div>
    <aside>
      Note that this goes a bit into the direction of so-called global explanations.
      However, we are speaking about explaining complex ML models to lay users who might not know anything about machine
      learning, so the educational intervention needs to be very high-level.
    </aside>

    <h3 id="study-conditions">Splitting (for) the Difference</h3>
    <p>
      Because it was unclear how these two different approaches, an educational intervention and individual explanations
      of results, would compare to each other, we decided to go with a 2&nbsp;&times;&nbsp;2 between-subjects design
      based on the two approaches.
      Essentially, this meant that we would first split all our participants into two groups. We would give one group
      the educational intervention (i.e., they got some basic info on how our AI worked in general), and the other
      group would not get anything at all.
      Then, we would split each of the two groups again, with one half of each group getting individual explanations of
      the AI suggestions, and the other half only getting the plain AI suggestions.
      This resulted in four different configurations: no extra information at all, educational intervention only,
      individual explanations only, and both types of extra information.
    </p>
    <p>
      While this meant that we had settled on our overall study design, it also meant that we could not vary any other
      parameters.
      Varying anything else would lead to a further subdivision of groups, requiring even more participants to obtain
      reliable results.
      We knew from the start that we wanted to present users with suggestions from an actual mushroom classification
      model, and not just with some fake outputs.
      Now it was up to the ML-focused members of our group to train such a model and decide on all its different
      parameters.
      In the end, we opted for a ResNet 50 model <d-cite key="he_deep_2016"></d-cite> pretrained on ImageNet data and
      fine-tuned on several thousand mushroom
      images that we collected from various sources (including the <em>Danish Svampeatlas</em> <d-cite
        key="danish_svampeatlas"></d-cite>).
      We also made sure that our model was bad enough to get some classifications wrong, as we wanted wrong AI
      suggestions to be reflected in the study.
    </p>
    <p>
      Our study design with the 2&nbsp;&times;&nbsp;2 split based on the types of explanatory information also meant
      that we only got one chance for each of the two.
      We had to fix one design for our educational intervention, and we had to fix one design for our XAI "interface".
      We use the term <em>interface</em> here for a reason.
      We decided that we wanted to show the results of our AI to the participants in the form of a fictitious app
      called <em>Forestly</em>.
    </p>
    <aside>
      <img src="images/forestly-logo.svg" width="100%" style="max-width: 200px;" />
      <p class="side-caption">
        Logo of the fictitious <em>Forestly</em> app.
      </p>
    </aside>
    <div>
      <p>
        This way, we tried to make the task of a joint human&ndash;AI mushroom classification in our experiment resemble
        that of a real-world scenario of going out into the forest with an app.
      </p>
      <p>
        On the side you can see a thumbnail of the educational intervention that we used in the study (click on it to
        expand it).
        Below are the two variants of the <em>Forestly</em> app, one with and one without XAI content.
        Hover over the boxes to receive extra infos on the components of the <em>Forestly</em>.
        Participants in the study also received this information, but in a static form.
      </p>
    </div>
    <aside>
      <img src="images/educational-intervention.svg" data-toggle="modal" data-target="#edu-modal"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2); max-width: 200px;" />
      <p class="side-caption">Educational intervention designed for the first study (click to expand).</p>
    </aside>
    <figure class="fullscreen">
      <div class="d-flex justify-content-center" id="forestly-interface" style="margin: 0 auto; width: 800px;"></div>
      <!-- <iframe width="100%" height="557" frameborder="0"
          src="https://observablehq.com/embed/@infovis2023/observable-tutorial-part-i@555?cells=pokeBar%2Cviewof+yMax"></iframe> -->
      <div class="d-flex justify-content-center">
        <figcaption>
          The <em>Forestly</em> interface used in the first study. Hover over the interface elements or the descriptions
          to reveal connecting lines.
        </figcaption>
      </div>
    </figure>
    <p>
      As you can see, we decided that both <em>Forestly</em> interfaces should show a ranking of predicted species.
      For each suggested species, we show percentages of likelihood (obtained through dropout during inference), and
      the
      ground truth edibility.
      We also show a combined edibility score based on these predictions in both variants.
      For the explanations, we opted for a combination of nearest neighbors and Grad-CAM <d-cite
        key="selvaraju_grad-cam_2017"></d-cite>.
      The nearest-neighbor explanation simply shows the nearest neighbor of the input image in the latent space of the
      model for each predicted species (similar to an approach used previously by Jeyakumar et al. <d-cite
        key="jeyakumar_how_2020"></d-cite>).
      The Grad-CAM explanation shows, well, Grad-CAM.
    </p>
    <h3 id="study-pretests">Tried and Tested</h3>
    <p>
      We had decided early on in our project that we were targeting a broad and diverse range of study participants.
      This meant that we could not expect any prior knowledge, neither about mushrooms nor about AI.
      To understand if the effects of explanations vary with different levels of prior knowledge about either topic, we
      constructed pretests.
    </p>
    <p>
      The first pretest collected information about the participants' mushroom knowledge.
      We created it in cooperation with mycologists from Austria and Germany.
      (potentially make mushroom pretest available here)
      The second pretest aimed at the participants' knowledge about AI.
      We created this one based on our own intuition and experience with communicating the topic to different audiences.
      (potentially make mushroom pretest available here)
      We also created a third test for what we called &ldquo;task-specific AI comprehension&rdquo;.
      This test, which was scheduled at the end of the study, asked questions about applying AI to mushroom
      identification.
      In hindsight, this test was probably not necessary, as it strongly correlated with the AI test&mdash;you can see
      that for yourself in a minute.
    </p>
    <p>
      We had to make sure that each test included questions spanning from easy to very hard.
      Only then could the tests fully cover the spectrum of knowledge levels in our broad participant population.
      We pretested the pretests with friends, colleagues, and relatives to make sure that this was the case.
    </p>
    <h3 id="study-stimuli">Stimulating!</h3>
    <p>
      Finally, we had to decide on the actual tasks and stimuli.
      Stimuli are the items that participants see during the study and which prompt them to perform a certain task.
      Earlier on, we mentioned that it makes sense for a user to trust the AI if its decision is correct, and distrust
      incorrect suggestions.
      Even though a user cannot know which is the case, we still wanted correct and incorrect AI predictions to be
      reflected among our items.
      Additionally, we thought it could make a big difference whether certain mushroom species were well known, and
      whether
      the AI's suggestion was &ldquo;edible&rdquo; or &ldquo;poisonous&rdquo;.
    </p>
    <p>
      In a laborious process, we browsed through hundreds of input images, predictions, and explanations, attempting to
      choose the ideal subset of stimuli.
      To increase the potential risk of incorrect classifications, we had limited ourselves to mushroom species that had
      known doppelgangers.
      You can see the images that we chose (for now, without any AI predictions) in the gallery below.
    </p>
    <d-figure>
      <figure class="l-body">
        <div id="mushroom-carousel" class="carousel slide" data-interval="false">
          <div class="carousel-inner">
            <div class="carousel-item active">
              <img class="d-block w-100" src="./images/mushrooms-study-1/FS2010PIC21012313.JPG"
                alt="Mushroom image 1 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/PO2009PIC35635771.JPG"
                alt="Mushroom image 2 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MIH2001-9191789_rJdZOCEkW.JPG"
                alt="Mushroom image 3 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/JC2017-9226499_r1lBDXQ03b.JPG"
                alt="Mushroom image 4 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TLP2017-9205443_BylavHGo_b.JPG"
                alt="Mushroom image 5 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TBE2017-9223269_S1xnszB2Z.JPG"
                alt="Mushroom image 6 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MSO2016-9181971_Hyi3GMJZl.JPG"
                alt="Mushroom image 7 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TAN2017-9216029_HJUDzpmsZ.JPG"
                alt="Mushroom image 8 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TS2011PIC24074671.JPG"
                alt="Mushroom image 9 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MMN2011PIC29734952.JPG"
                alt="Mushroom image 10 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/OBL2010PIC84482004.JPG"
                alt="Mushroom image 11 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/OMA2017-9202807_ByLKZi2P-.JPG"
                alt="Mushroom image 12 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/MCE2017-9207165_ryEbbxXYb.JPG"
                alt="Mushroom image 13 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/UFN2017-9197245_SkzMDHjA4-.JPG"
                alt="Mushroom image 14 used in the first study">
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/mushrooms-study-1/TS2014PIC25950997.JPG"
                alt="Mushroom image 15 used in the first study">
            </div>
          </div>
          <a class="carousel-control-prev" href="#mushroom-carousel" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#mushroom-carousel" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </figure>
    </d-figure>
    <aside class="figcaption">
      Mushroom images used in the first study (click left or right to browse).
    </aside>
    <p>
      We told our participants to imagine going out into the forest to collect mushrooms for a nice stew with the help
      of the <em>Forestly</em> app.
      We showed the mushroom images and the AI predictions in the form of the <em>Forestly</em> interface to the
      participants and asked them two questions for each item:
    </p>
    <ul>
      <li>Would you classify this mushroom as edible or inedible/poisonous?</li>
      <li>Would you pick up this mushroom and use it for cooking?</li>
    </ul>
    <p>
      We split the question into two parts because we thought that it would be interesting to see if participants were
      more conservative in their choices when it came to actually picking the mushroom versus superficially gauging its
      edibility.
      After each item, we also asked participants how much they trusted the app.
    </p>
    <p>
      After both pretests and the actual mushroom classification, we concluded the experiment with the
      &ldquo;task-specific
      AI comprehension&rdquo; test and general questions about the participants' intention to use the <em>Forestly</em>
      app or similar systems.
    </p>
    <h3 id="study-evaluation">Crunching the Numbers</h3>
    <p>
      We recruited 410 participants for this first experiment, which was carried out as an online study.
      This number was based on an a priori power analysis (something that the psychologist team members brought to the
      table).
      We also performed a quantitative analysis of the results based on all the best practices established by
      psychologists.
      These are quite a bit more rigorous than what you would usually see in a visualization or ML paper with user
      studies.
      Below we summarize the most important results, but you can also play around with the interactive plotting tool to
      explore different aspects of our data.
      For example, you can <a href="#study-1-plot" id="test-corr">verify the correlation between the results of the AI
        knowledge test and those of the &ldquo;task-specific AI comprehension&rdquo; test</a>.
    </p>

    <figure class="fullscreen-diagram">
      <div class="d-flex flex-row justify-content-center align-items-center">
        <div class="d-flex flex-column" style="width: 30%; margin-right: 10%;">
          <h4>Results from the First Study</h4>
          <ul>
            <li>
              Our educational intervention had no statistically significant effect. Participants who received it did not
              perform any differently in terms of decision correctness.
            </li>
            <li>
              XAI did affect the performance. Participants who received explanations along with the AI suggestions
              made
              significantly more correct decisions (i.e., they knew better when to trust the AI and when not).
            </li>
            <li>
              At the same time, participants who saw the XAI interface reported less trust in the system.
            </li>
            <li>
              Decision performance did not vary with AI knowledge, and&mdash;more interestingly&mdash;did not vary
              with mushroom knowledge.
            </li>
          </ul>
        </div>
        <div class="d-flex flex-column" style="width: 460px;">
          <div id='study-1-plot' style="margin-top: 0px;"></div>
          <div class="figcaption" id='study-1-x'></div>
          <div class="figcaption" id='study-1-y'></div>
        </div>
      </div>
    </figure>

    <h2 id="ars-study">Going Artistic</h2>
    <div class="card text-white" style="width: 100%; background-color: hsl(200, 60%, 15%); margin-bottom: 2ex;">
      <img class="card-img-top" src="images/pexels-photo-2952871-cropped.jpg" alt="Book shelf">
      <div class="card-body">
        <a href="https://doi.org/10.1080/10447318.2023.2221605" class="stretched-link"></a>
        <h4 class="card-title">
          Explainable Artificial Intelligence improves human decision-making: Results from a mushroom picking experiment
          at a public art festival
        </h4>
        <p class="card-text">
          Benedikt Leichtmann, Andreas Hinterreiter, Christina Humer, Marc Streit, Martina Mara.
        </p>
        <p class="card-text">
          International Journal of Human&ndash;Computer Interaction, DOI: 10.1080/10447318.2023.2221605, 2023.
        </p>
      </div>
    </div>
    <p>
      Every year, the city of Linz in Austria is home to the <a href="https://ars.electronica.art/festival/en/">Ars
        Electronica Festival</a>&mdash;a festival for art, technology, and society.
      For several years, the campus of our university (<a href="https://www.jku.at/">Johannes Kepler University</a>) was
      one of the main sites of the festival.
      In those years, university members could apply for an exhibition site to showcase artistic aspects of their
      research projects.
      In the <a href="https://ars.electronica.art/newdigitaldeal/en/">2021 edition</a>, we used this opportunity to be
      present with our research project.
    </p>
    <p>
      We created a concept for an installation called <em>AI Forest</em> that would serve as a hybrid between an
      artistic narrative space and a research environment.
      Our goal was to check if we could reproduce results from the first study with festival visitors in an engaging
      setting.
      To this end, we came up with two extensions to our first study:
    <ol>
      <li>We wanted to use <em>gamification</em> to make the mushroom classification task more exciting and motivating.
      </li>
      <li>We wanted visitors to feel closer to an actual mushroom hunting trip to the forest.</li>
    </ol>
    </p>

    <h3 id="ars-game">I Want to Play a Game</h3>

    <p>
      For the first parts, the gamification, we decided to embed the survey from the first study in a game optimized for
      tablet computers.
      The story of the game was similar to our motivating scenario mentioned earlier: the game was about a mushroom hunt
      with the intention to collect enough mushrooms for a nice stew.
      Obviously, only edible mushrooms should be collected.
      After shortened versions of our pretests, visitors of our installation would search for physical mushroom models
      in the <em>AI Forest</em>.
      They would scan these models to be presented with the individual classification tasks from the study.
      Here, we used the same two versions of <em>Forestly</em> as previously.
      We got rid of the education intervention, because that did not work in our first study.
      As stimuli, we used the subset of 10 mushrooms that had the most variance.
      Through the shortened pretests and the fewer classification items, we could cut down the completion time of the
      survey.
      This was necessary, because festival visitors naturally could not spend too much time at our installation, and the
      newly introduced task of searching for the mushrooms would also take some time.
      After completing the game (i.e., finding and scanning 10 mushrooms), visitors would get feedback on their result.
      Below you can see the different outcomes of the game.
    </p>
    <figure id="endscreen-figure" class="fullscreen d-flex justify-content-around">
      <div class="d-flex flex-column" style="width: 20%"">
        <img src=" ./images/Endscenario-1.png"
        alt="End screen for players collecting all edible mushrooms and no poisonous ones.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            Success! The player collected all the edible mushrooms and no poisonous ones. The stew turned out
            great!
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 20%">
        <img src="./images/Endscenario-2.png"
          alt="End screen for players collecting some edible mushrooms and no poisonous ones.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            Pretty good. The player collected some edible mushrooms and no poisonous ones. The stew turned out fine.
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 20%">
        <img src="./images/Endscenario-3.png" alt="End screen for players collecting at least one poisonous mushroom.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            Oh no! The player collected at least one poisonous mushroom. Maybe it would be better to order some pizza
            instead?
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 20%;">
        <img src="./images/Endscenario-4.png" alt="End screen for players collecting no mushrooms at all.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            Hmm ... The player did not collect any mushrooms at all. The dinners guests will stay hungry.
          </figcaption>
        </div>
      </div>
    </figure>
    <p>
      <a href="#ai-forest-game">Later in this article</a> you can play an updated version of the game yourself.
      But first we want to give you an impression of where our festival visitors played their version of the game.
    </p>

    <h3 id="ars-forest">Wood Chips & Fragrance Dispensers</h3>

    <p>
      The <em>AI Forest</em> installation consisted of an artificial forest envisioned by German artist Birke van
      Maartens.
      It was constructed by a team of stage builders from various materials.
      Pedestals made from wood and vinyl served as &ldquo;islands&rdquo; on which different plants were placed.
      In total, we used 80 potted ivy and fern plants and around 30 large tree branches, which emulated trees.
      Visitors, each holding a tablet computer with the game installed, walked between these islands on wood chips.
      Hidden throughout the forest were not only the mushroom models&mdash;which had QR codes printed on them, so that
      they could be scanned with the tablets&mdash;but also speakers and fragrance dispensers.
      The speakers played bird songs from Austrian bird species, rustling wind noises, and one played the sounds of a
      nearby stream of water.
      These sounds were specially curated by audio artist Kenji Tanaka.
      The fragrance dispensers released vapor smelling of fir, spruce, and different pine trees.
      Below you can get some impressions of how the whole installation looked like.
    </p>

    <d-figure>
      <figure class="l-body">
        <div id="aec-carousel" class="carousel slide" data-interval="false">
          <div class="carousel-inner">
            <div class="carousel-item active">
              <img class="d-block w-100" src="./images/2021_AIForest_ForestOutside_Voggeneder.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Outside view of the AI Forest installation (© vog.photo/Ars Electronica,
                  licensed under CC By-NC-ND
                  2.0).</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_ForestInside_Leichtmann.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Study participants playing the game inside the installation.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_Mushroom_Leichtmann.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Some mushroom models were 'hidden' in plain sight.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_Scan_Voggeneder.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">This made them easy to scan for the participants ... (© vog.photo/Ars
                  Electronica, licensed under CC
                  By-NC-ND 2.0).</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2021_AIForest_VisitorPlaying_Ars.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">... as seen here.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/2022_AIForest_Floorplan_BirkeVanMaartensC.png" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Sketch of the forest installation as conceived by the artist.</div>
              </div>
            </div>
            <div class="carousel-item">
              <img class="d-block w-100" src="./images/AIForest_picturefromabove.jpg" alt="First slide">
              <div class="carousel-caption d-none d-md-block">
                <div class="figcaption">Aerial view showing how the 'islands' were realized in the end.</div>
              </div>
            </div>
          </div>
          <a class="carousel-control-prev" href="#aec-carousel" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#aec-carousel" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </figure>
    </d-figure>
    <p>
      Throughout our three-day presence at the Ars Electronica Festival, we welcomed 466 visitors to our <em>AI
        Forest</em>.
      We could use the study data from 328 of these participants.
      We did not use data from participants who played the game together (e.g., parents with their children), and some
      visitors could not finish the entire game due to time constraints of their visit.
      In addition to the study data, we also got a lot of feedback from informal conversations with people after their
      visits.
      Below is a summary of the results from our artistic endeavor.
    </p>

    <figure class="fullscreen-diagram">
      <div class="d-flex flex-row justify-content-center align-items-center">
        <div class="d-flex flex-column" style="width: 30%;">
          <h4>Results from the Festival Study</h4>
          <ul>
            <li>
              We could reproduce the main result from our first study. Participants who got the XAI interface again
              performed better in terms of classification correctness than participants who got the plain interface.
              This is true for both the edibility assessment and the actual pick-up decision (remember that we split
              the identification task up into two questions.)
            </li>
            <li>
              We could not reproduce our previous results regarding self-reported trust. This time, participants who
              got the XAI interface did not report to trust the AI significantly less. Note, however, that in the
              festival study we only asked about the self-reported trust once at the end, and not for every item
              individually. We had changed this to keep the completion time short.
            </li>
            <li>
              Along a similar line, participants who got the XAI interface did not rate the <em>Forestly</em> app in
              a significantly different way.
            </li>
            <li>
              Informal discussions after the game suggested that visitors really liked our installation, but also that
              many were not aware of the dangers of using AI systems for high-risk decisionmaking tasks.
            </li>
          </ul>
        </div>
      </div>
    </figure>

    <h2 id="study-3">What's the Difference?</h2>
    <div class="card text-white" style="width: 100%; background-color: hsl(200, 60%, 15%); margin-bottom: 2ex;">
      <img class="card-img-top" src="images/pexels-photo-2952871-cropped.jpg" alt="Book shelf">
      <div class="card-body">
        <a href="https://doi.org/10.31219/osf.io/h6dwz" class="stretched-link"></a>
        <h4 class="card-title">
          Comparing Effects of Attribution-based, Example-based, and Feature-based Explanation Methods on AI-Assisted
          Decision-Making
        </h4>
        <p class="card-text">
          Christina Humer, Andreas Hinterreiter, Benedikt Leichtmann, Martina Mara, Marc Streit.
        </p>
        <p class="card-text">
          OSF Preprint, DOI: 10.31219/osf.io/h6dwz, 2022.
        </p>
      </div>
    </div>

    <p>
      After our first online study and the gamified replication study at the festival, we now had reliable results about
      our XAI interface helping users to make more correct decisions (i.e., to trust the AI more adequately).
      We mentioned <a href="#study-conditions">earlier</a> that when we designed our XAI interface, we opted for a
      combination of nearest neighbors and Grad-CAM.
      While we did know now that this combination worked, we did not know if that was because nearest neighbors worked,
      Grad-CAM worked, or the combination worked.
      We set out to perform a third study with new <em>Forestly</em> interfaces, in each of which only <em>one</em>
      explanation technique was used.
      This also gave us the opportunity to study an additional explanation technique.
    </p>

    <h3 id="study-3-dissect">A New Challenger Has Appeared</h3>

    <p>
      From informal feedback by the participants of our festival study, we got the feeling that most people had a hard
      time understanding and using Grad-CAM.
      Most of the participants from the XAI group told us that they had looked mostly at the nearest-neighbor images.
      We hoped to create a third explanation technique, which would improve upon Grad-CAM by not only showing an
      attribution map, but also relating the map to concepts tied to mushrooms.
      For this new technique, we took inspiration from the network dissection technique <d-cite
        key="bau_network_2017,bau_understanding_2020"></d-cite>.
      We calculated the most important units of our neural network for each classification.
      We then assigned semantic labels to the units and showed the activation maps of the units along with the labels.
      With this new interface variant, we now had four <em>Forestly</em> variants: no explanations, nearest neighbors
      (now without Grad-CAM), Grad-CAM (now without nearest neighbors), and the new semantic explanations.
    </p>
    <figure class="fullscreen d-flex justify-content-around">
      <div class="d-flex flex-column" style="width: 17%">
        <img src="./images/onboarding-noexpl_EN.png" alt="Forestly variant without any explanations.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style=" text-align: center;">
            Control interface with no explanations
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 17%">
        <img src="./images/onboarding-nn_EN.png" alt="Forestly variant with nearest neighbor explanations.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            <em>Forestly</em> interface with nearest-neighbor explanations.
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 17%">
        <img src="./images/onboarding-gradcam_EN.png" alt="Forestly variant with Grad-CAM explanations.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            <em>Forestly</em> interface with Grad-CAM explanations.
          </figcaption>
        </div>
      </div>
      <div class="d-flex flex-column" style="width: 17%;">
        <img src="./images/onboarding-dissect_EN.png" alt="Forestly variant with semantic explanations.">
        <div class="d-flex justify-content-center subcaption">
          <figcaption style="text-align: center;">
            <em>Forestly</em> interface with semantic explanations.
          </figcaption>
        </div>
      </div>
    </figure>

    <p>
      In our third study, we divided our participants into four groups. Each group received one of the four
      layout variants shown above. But there was one more thing we wanted to explore in our study ...
    </p>

    <h3 id="study-3-align">Something's Awry</h3>

    <p>
      Remember when we mentioned the <a href="#study-stimuli">painstaking process of selecting certain mushroom
        images</a> as stimuli? One thing we noticed while browsing through hundreds of mushroom classifications and
      their explanations was that sometimes an explanation could be &ldquo;reassuring&ldquo;. It could make you think
      &ldquo;Well, looks like everything makes sense here.&ldquo; At other times, looking at an explanation could raise
      some concerns and make you think &ldquo;Hmm, something's not quite right here.&ldquo; For example, a Grad-CAM
      explanation could highlight only strange regions in the background of the mushroom image. Would you trust an AI
      suggestion if the AI did not even focus on the mushroom itself?
    </p>
    <p>
      Our rough understanding was that &ldquo;reassuring&ldquo; explanations would be helpful if the AI was correct.
      The other kind of explanations, which made things seem a bit more shady, would be more helpful in cases when the
      AI was incorrect. You could then realize that something maybe wasn't quite right and overrule the AI.
      For our selection of mushroom stimuli, we classified the explanations into the two different types and made sure
      to include some items of each kind.
    </p>
    <p>
      Before we get to our classification and the experimental results, you can play the game that our participants
      played in this third study. We liked the gamification from the festival study, so we decided to reuse the game
      idea and update the game with the new interfaces:
    </p>
    <figure class="fullscreen">
      <div id="ai-forest-game" class="d-flex flex-column">
        <div class="d-flex justify-content-center">
          <iframe style="width: 80vw; height: 45vw;" frameborder="0" src="https://jku-vds-lab.at/hoxai/"></iframe>
        </div>
        <div class="d-flex justify-content-center">
          <figcaption style="margin-top: 10px;">
            Variant of our mushroom hunting game used for the third study.
            <!-- TODO: update game -->
          </figcaption>
        </div>
      </div>
    </figure>
    <p>
      We conducted the third study as an online experiment with 501 participants.
      Below are the summarized insights of the study, along with an interactive widget for exploring the quantitative
      results.
      In the table, each row lists one of the mushroom items from the study, with indications for whether the AI
      suggestion was correct (<img class="inline-icon" src="./images/icon-ai-correct.png" alt="AI correct icon" />) or
      incorrect (<img class="inline-icon" src="./images/icon-ai-incorrect.png" alt="AI incorrect icon" />), whether the
      mushroom depicted was actually
      edible (<img class="inline-icon" src="./images/icon-edible.png" alt="Edible icon" />) or poisonous (<img
        class="inline-icon" src="./images/icon-poisonous.png" alt="Poisonous icon" />), and whether we deemed the
      explanation to be
      reassuring (<img class="inline-icon" src="./images/icon-xai-aligned.png" alt="Explanation reassuring icon" />; the
      explanation fits with AI suggestion like two pieces of a puzzle) or to raise concerns (<img class="inline-icon"
        src="./images/icon-xai-notaligned.png" alt="Explanation raises concerns icon" />).
      The two rightmost columns of the table show how many participants assumed the mushroom to be edible and how many
      picked it up, respectively.
      The bar chart in the top left of the widget shows the average correctness of the pick-up decision faceted by the
      four experimental groups and by the selection in the table.
      Clicking on the button in the head of a row facets the results by the value of that row. For example, if you
      click &ldquo;AI correctness&rdquo;, one bar for each group will show the correctness for only the mushroom items
      with an incorrect AI suggestion, and one bar for the items with correct AI suggestions.
      You can hover over the image thumbnails to view the <em>Forestly</em> screens that the control group saw.
      Hovering over XAI icons (<img class="inline-icon" src="./images/icon-xai-aligned.png"
        alt="Explanation reassuring icon" style="height: 2ex; margin-left: 0.16em; margin-right: 0.16em;" /> or <img
        class="inline-icon" src="./images/icon-xai-notaligned.png" alt="Explanation raises concerns icon" />) shows the
      screens for the respective variant. Finally, you can create your own item split by simply selecting items in the
      leftmost column of the table.
      For example, select the two items with a concerning nearest-neighbor explanation and an incorrect AI suggestion,
      and notice how much higher the correctness was for those items for the nearest-neighbor group.
    </p>
    <figure class="fullscreen-diagram">
      <div class="d-flex flex-row justify-content-center align-items-center">
        <div class="d-flex flex-column" style="width: 25%; margin-right: 5%;">
          <h4>Results from the Third Study</h4>
          <ul>
            <li>
              Nearest-neighbor explanations were the only ones that significantly improved the decision correctness.
              The other two explanation techniques, Grad-CAM and our newly introduced semantic explanation, failed to
              do so.
              This means that the effectiveness of our previous combined explanation interface most likely hinged on
              the nearest-neighbor explanations.
            </li>
            <li>
              Nearest neighbors also was the only technique that led to significant differences for the subset of
              items with incorrect AI suggestions.
            </li>
            <li>
              Individual explanations can strongly affect the user behavior.
              We found certain items for which the average behavior of one participant group deviated strongly from
              the others.
              Again, this effect was especially strong for one of the nearest-neighbor explanations.
            </li>
            <li>
              Helpful explanations (which we defined as <img class="inline-icon" src="./images/icon-ai-correct.png"
                alt="AI correct icon" />+<img class="inline-icon" src="./images/icon-xai-aligned.png"
                alt="Explanation reassuring icon" /> or <img class="inline-icon" src="./images/icon-ai-incorrect.png"
                alt="AI incorrect icon" />+<img class="inline-icon" src="./images/icon-xai-notaligned.png"
                alt="Explanation raises concerns icon" />) led to better performance (i.e., more adequate trust).
              This was again only statistically significant for nearest neighbors, but a similar tendency was
              observable for the semantic explanation.
            </li>
          </ul>
        </div>
        <div class="d-flex flex-column" style="width: 1000px">
          <div id="study-3-plot"></div>
          <div id="study-3-table"></div>
        </div>
      </div>
    </figure>

    <h2 id="blaming">Who's to Blame?</h2>

    <p>
      In <a href="#endscreen-figure">one of our game's end screens</a>&mdash;the one for the case of at least one
      poisonous mushroom picked up&mdash;the toxicity of the stew is apparent. In real life, unfortunately, a poisonous
      stew does not emit green vapor and skull symbols. On the contrary, for many mushrooms you might not taste any
      difference and only find out when it's too late. If you collected the mushrooms with AI assistance and ended up
      poisoning one of your dinner guests, who would you blame?
    </p>
    <p>
      This is exactly the question that interested us in a final study.
      We had sneakily included the following scenario in the surveys of the two online studies:
    <blockquote>
      Assume you decide to take a mushroom with you based on a recommendation of the artificial intelligence and give
      it to a friend to eat. It turns out that it was a poisonous mushroom, and your friend complains of nausea,
      vomiting and diarrhea.
    </blockquote>
    <p>
      We then asked the participants how much each agent in this scenario was to blame.
      The different agents here are the participant themself, the AI, the AI's developers, the poisoned friend.
      In addition to the blaming questions, we also asked participants in one of the studies how they viewed
      artificially intelligent agents in general.
      These are the results:
    </p>
    <figure class="fullscreen-diagram">
      <div class="d-flex flex-row justify-content-center align-items-center">
        <div class="d-flex flex-column" style="width: 25%;">
          <h4>Results on Blaming</h4>
          <ul>
            <li>
              In the first online study, participants who did not receive additional explanations along with the AI
              suggestions blamed
              the AI significantly more.
            </li>
            <li>
              In the second online study, there were no statistically significant effects, but results for the nearest
              neighbor group suggest a trend towards participants blaming themselves more.
            </li>
            <li>
              Participants blamed the AI more if they perceived it as having a mind of its own (which is in line with
              previous psychological studies <d-cite key="gray_mind_2012"></d-cite>). Participants also blamed
              themselves more if they thought they could have recognized the false classification (again in line with
              previous research <d-cite key="malle_theory_2014"></d-cite>). This can also be seen in the network model
              of moral reasoning shown on the right.
            </li>
            <li>
              Across both studies, we found only limited evidence for <em>scapegoating</em>, i.e., there was no
              negative correlation between blaming oneself and blaming other actors.
            </li>
            <li>
              Blame is not distributed equally. People blamed themselves and the AI most, followed by the developers.
              The friend received the lowest amount of blame.
            </li>
          </ul>
        </div>
        <div class="d-flex flex-column" style="width: 600px">
          <div id="blaming-legend"></div>
          <div id="blaming-network"></div>
          <div>
            <figcaption>
              Network model of moral reasoning in the fictitious poisoning scenario. Hover over the nodes to find out
              what they stand for.
            </figcaption>
          </div>
        </div>
      </div>
    </figure>

    <p>
      In addition to the results above, we also got some interesting takes on the scenario.
      One participant felt that they are not to blame because
    </p>
    <blockquote>
      ... [the AI] told me to pick the mushroom.
    </blockquote>
    <p>
      Another participant outright blamed their poor poisoned friend
    </p>
    <blockquote>
      ... because he himself should have checked what he eats.
    </blockquote>
    <p>
      You might want to think about these viewpoints the next time you visit one of your friends for a dinner night with
      dishes made from foraged mushrooms.
    </p>

    <h2 id="conclusion">Conclusions</h2>

    <p>
      Finally, we want to summarize some of our main insights from this multi-year, transdisciplinary research project.
    </p>
    <p>
      <b>Replication studies are important.</b> Throughout the different studies, we tried to replicate several
      findings. The festival study was a replication study of the first experiment in a different setting. The second
      online study as well as the second part of the blaming study aimed at relating the effects observed for the
      combined interface to the individual explanation techniques. In all cases, we found that certain effects could be
      replicated, while others might have been false positives or tied to very specific study conditions.
      This allowed us to obtain reliable results.
      We encourage all readers to consider replication studies for their own results as well as those of others.
      While the psychology research community has long accepted the importance of such replication studies, we hope that
      the visualization community will follow suit.
    </p>
    <p>
      <b>There's much more to do.</b> Designing user studies is difficult and comes with many, many choices.
      A single study, or even a whole research project, can only focus on a few dependent and independent variables.
      All others have to be left fixed as control variables.
      In our experiments, this meant that we only used one specific model with a fixed accuracy.
      However, the accuracy of the model (and if and how you communicate it to the user) might be an important mediating
      factor for how much users trust an AI.
      There are many more potential mediating factors.
      There are also many more application scenarios apart from mushrooms.
      It will be interesting to see how our results generalize in future studies.
    </p>
    <p>
      <b>It's fun to try new things.</b> We started this project as a team consisting of computer scientists and
      psychologists.
      In the course of the project, we got to work with mycologists, graphic designers, visual and audio artists and
      stage builders.
      Some of us also got to be guides for an art festival installation, talking to hundreds of visitors with all sorts
      of personal backgrounds.
      All of these interactions were incredibly rewarding and it was great fun to combine so many influences in a
      research project.
    </p>
    <p>
      <b>We don't have to become Erewhonians.</b> Our results strongly suggest that XAI can be one way to help users
      towards a more responsible use of AI support systems.
      Correctly designed explanations can lead users to trusting an AI more adequately.
      As a result, we believe that XAI can and will play a big role in the future for trust calibration.
      The Erewhonians chose to relinquish machines altogether because they saw too many risks.
      But if people learn when to trust and when not to trust an AI, we will be able to keep artificially intelligent
      systems as effective tools in our society.
    </p>

  </d-article>

  <d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>

  <!-- Modal for Educational Intervention-->
  <div class="modal fade" id="edu-modal" tabindex="-1" aria-labelledby="EducationalInterventionModalLabel"
    aria-hidden="true">
    <div class="modal-dialog modal-xl">
      <div class="modal-content modal-xl">
        <img src="images/educational-intervention.svg" style="width:100%" />
      </div>
    </div>
  </div>

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>

  <script type="module">
    import define from "./notebook/index.js";
    import { Runtime, Library, Inspector } from "./notebook/runtime.js";

    const runtime = new Runtime(Object.assign(new Library, { width: 1152 }));

    const main = runtime.module(define, name => {
      switch (name) {
        case "viewof toggleHideShow": {
          return new Inspector(document.querySelector("#doppelganger-checkbox"))
        };
        case "updateDoppelganger": {
          return true
        };
        case "mushroomDoppelganger": {
          return {
            fulfilled(value) {
              value.removeAttribute("width");
              value.removeAttribute("height");
              document.querySelector("#doppelganger-main").replaceChildren(value);
            }
          };
          // return new Inspector(document.querySelector("#doppelganger-main"))
        };
        case "forestlyInterface": {
          return new Inspector(document.querySelector("#forestly-interface"))
        }
        case "viewof selectedXAxis": {
          return new Inspector(document.querySelector("#study-1-x"))
        };
        case "viewof selectedYAxis": {
          return new Inspector(document.querySelector("#study-1-y"))
        };
        case "plotAnyTwo": {
          return {
            fulfilled(value) {
              const children = value.childNodes;
              if (children.length == 1) {
                const barChart = children[0]
                barChart.setAttribute("style", "background: none;")
                barChart.removeChild(barChart.getElementsByTagName("style")[0]);
              } else {
                const legend = children[0]
                legend.removeChild(legend.getElementsByTagName("style")[0]);
                const heatmap = children[1]
                heatmap.setAttribute("style", "background: none;")
                heatmap.removeChild(heatmap.getElementsByTagName("style")[0]);
              };
              console.log(children.length);
              [].forEach.call(children, function (child) {
                child.removeAttribute("width");
                child.removeAttribute("height");
              });
              document.querySelector("#study-1-plot").replaceChildren(value);
            }
          };
          // return new Inspector(document.querySelector("#study-1-plot"))
        };
        case "blamingLegend": {
          return new Inspector(document.querySelector("#blaming-legend"))
        };
        case "blamingNetwork": {
          return new Inspector(document.querySelector("#blaming-network"))
        };
        case "plotAndImage": {
          return new Inspector(document.querySelector("#study-3-plot"))
        };
        case "viewof mushroomItems": {
          return new Inspector(document.querySelector("#study-3-table"))
        };
      }
    });

    document.getElementById("test-corr").addEventListener("click", setTestAxes, false);

    function setTestAxes() {
      const selectX = document.getElementById("oi-66c06c-2")
      selectX.value = 10;
      selectX.dispatchEvent(new Event("input", { bubbles: true }));
      const selectY = document.getElementById("oi-66c06c-1")
      selectY.value = 8;
      selectY.dispatchEvent(new Event("input", { bubbles: true }));
    }

  </script>

</body>